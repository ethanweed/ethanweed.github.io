---
title: "Modelling the Stroop Data"
author: "Ethan Weed"
date: "9/17/2019"
header-includes:
   - \usepackage{bbm}
output: 
  html_document:
    df_print: default
    code_folding: hide
    number_sections: no
    self_contained: yes
    toc: no
    toc_depth: 3
    toc_float:
      collapsed: no
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo = T, results = 'false', message=FALSE, warning = F}




# Load packages for data handling and plotting
library(tidyverse)

# clear global environment (get rid of any old variables that might be hangning around)
rm(list = ls())

# read in the data
df <- read.csv("/Users/ethan/Documents/GitHub/ethanweed.github.io/r-tutorials/data/Stroop-raw-over-the-years.csv")

# make a new dataframe with the same data in the "long" format.
#df <- gather(df, key = "Condition", value = "Time", -Year)

column_means <- colMeans(df)

df$points <- seq(1:nrow(df))

RNI <- ggplot(data = df, aes(x = points, y = Reading_NoInt)) +
  geom_point() + 
  theme_classic() +
  geom_hline(yintercept = column_means[1]) +
  ylim(0, 18)

NI <- ggplot(data = df, aes(x = points, y = Naming_Int)) +
  geom_point() + 
  theme_classic() +
  geom_hline(yintercept = column_means[2]) +
  ylim(0, 18)

NNI <- ggplot(data = df, aes(x = points, y = Naming_NoInt)) +
  geom_point() + 
  theme_classic() +
  geom_hline(yintercept = column_means[3]) +
  ylim(0, 18)
              
RI <- ggplot(data = df, aes(x = points, y = Reading_Int)) +
  geom_point() + 
  theme_classic() +
  geom_hline(yintercept = column_means[4])+
  ylim(0, 18)

library(ggpubr)

ggarrange(RI, RNI, NI, NNI, ncol = 2, nrow = 2)
```

The variation between the individual data points and the model (the mean) can be described as the standard deviation.
 $$ s = \sqrt{\frac{\sum_{i=1}^N (x_i - \bar{x})^2}{N-1} } $$
In this equation, $\textstyle\{x_1,\,x_2,\,\ldots,\,x_N\}$ are the observed values of the sample items, $\textstyle\bar{x}$ is the mean value of these observations, and *N* is the number of observations in the sample.


## "t-test" 

Let's just concetrate on conditions with interference. Is there a significant difference between **Reading** and **Naming**?

```{r}
ggarrange(RI, NI, ncol = 2)
```


The data from the two conditions look different. But are they really? Put differently: if we model these data as being drawn from two different distributions, does this two-distribution model better describe the data than the alternative, that is, that all the data points are simply drawn from the same distribution?


If we look at the distribution of all the **Reading with Interference** and **Naming with Interference** data points, it looks like this:
```{r echo = T, results = 'false', message=FALSE, warning = F}
library(ggpubr)
df_long <- gather(df, key = "Condition", value = "Time", -Year)
df_int <- subset(df_long, Condition == "Reading_Int" | Condition == "Naming_Int")
df_int$Condition <- as.factor(df_int$Condition)

p1 <- ggplot(df_int, aes(Time)) +
  geom_density() +
  theme_classic() +
  labs(title = "Single distribution model")

p1

```


How would these same data look, if we modelled them as coming from two different distributions?
```{r echo = T, results = 'false', message=FALSE, warning = F}
# make individual plots for different conditions
p2 <- ggplot(df_int, aes(Time, fill = Condition)) +
  geom_density(alpha = 0.5) +
  theme_classic() +
  labs(title = "Two-distribution model")
p2
```

There is some overlap in data, but the two distributions do appear to be distinct. It looks as if the 2-condition model does fit the data relatively well.
```{r echo = T, results = 'false', message=FALSE, warning = F}

ggarrange(p1, p2, ncol = 2)

```

This comparison of distributions can be formalized as the "t-test". In this case, we use a *paired t-test*, since the same participants were tested in both conditions. Other t-tests include the *independent samples* t-test for testing e.g. between two different groups of participants, and the *single-sample* t-test, for testing whether a sample mean is significantly different from a specified number, e.g. different than zero.

```{r echo = T, results = 'false', message=FALSE, warning = F}
library(pander)
Reading <- subset(df_int$Time, df_int$Condition == "Reading_Int")
Naming <- subset(df_int$Time, df_int$Condition == "Naming_Int")
t <- t.test(Reading, Naming, paired = TRUE)
pander(t)
```


We should be able to take random samples from the same overall distribution and see the same pattern most of the time. Here are 10 random re-samples of 100 data points each from total pool of data.
```{r echo = T, results = 'false', message=FALSE, warning = F}
set.seed(42)
for (i in 1:10) {
  print(paste0("Sample ", i))
  d <- df_int[sample(nrow(df_int), 100), ]
  print(ggplot(d, aes(Time, fill = Condition)) +
  geom_density(alpha = 0.5) +
  theme_classic()) 
}


```

## "ANOVA"

```{r echo = T, results = 'false', message=FALSE, warning = F}

# make individual plots for different conditions
df_long$Condition <- as.factor(df_long$Condition)
df_long <- subset(df_long, Condition != "points")


p1 <- ggplot(df_long, aes(Time)) +
  geom_density() +
  theme_classic() +
  labs(title = "All Data")

p2 <- ggplot(df_long, aes(Time, fill = Condition)) +
  geom_density(alpha = 0.5) +
  theme_classic() +
  labs(title = "Data Modelled by Condition")

ggarrange(p1, p2, ncol = 1, nrow = 2)

```



```{r echo = F, results = 'false', message=FALSE, warning = F}




df_long <- gather(df, key = "Condition", value = "Time", -Year, -points)
df_long$Task <- ifelse(df_long$Condition == 'Reading_Int' | df_long$Condition == 'Reading_NoInt', "Reading", "Naming")
df_long$Interference <- ifelse(df_long$Condition == 'Reading_Int' | df_long$Condition == 'Naming_Int', 'Interference', 'No_Interference')
df_long$ID <- as.factor(df_long$points)
df_long$measurements <- seq(1:nrow(df_long))

CM <- aggregate(df_long$Time, list(df_long$Condition), mean)
CM <- CM$x

p4 <- ggplot(data = df_long, aes(x = points, y = Time, color = Condition)) +
  geom_point() + 
  geom_hline(yintercept = CM[1], linetype = "dashed") +
  geom_hline(yintercept = CM[2]) +
  geom_hline(yintercept = CM[3], linetype = "dashed") +
  geom_hline(yintercept = CM[4]) +
  geom_label(label="NI", x = 40, y = CM[1], color = "black") +
  geom_label(label="NNI", x = 60, y = CM[2], color = "black") +
  geom_label(label="RI", x = 80, y = CM[3], color = "black") +
  geom_label(label="RNI", x = 90, y = CM[4], color = "black") +
  theme_classic()
p4
```
  

```{r echo = F, results = 'false', message=FALSE, warning = F}
p3 <- ggplot(df_long, aes(Time, fill = Condition)) +
  geom_density(alpha = 0.5) +
  theme_classic() +
  coord_flip() +
  labs(title = "Data Modelled by Condition")
p4 <- p4 + labs(title = "Data Modelled by Condition")
ggarrange(p3, p4, ncol = 2)

```

  
  
```{r echo = T, results = 'false', message=FALSE, warning = F}  

# Find mean of all data points
GrandMean <- mean(df_long$Time)

# Find means for each condition
meanNI <- mean(df$Naming_Int)
meanRI <- mean(df$Reading_Int)
meanNNI <- mean(df$Naming_NoInt) 
meanRNI <- mean(df$Reading_NoInt)

# Get x-axis values for each data point, grouped by condition
xNI <- subset(df_long$measurements, df_long$Condition == "Naming_Int")
xRI <- subset(df_long$measurements, df_long$Condition == "Reading_Int")
xNNI <- subset(df_long$measurements, df_long$Condition == "Naming_NoInt")
xRNI <- subset(df_long$measurements, df_long$Condition == "Reading_NoInt")


#yNI <- subset(df_long$Time, df_long$Condition == "Naming_Int") 
#yRI <- subset(df_long$Time, df_long$Condition == "Reading_Int")
#yNNI <- subset(df_long$Time, df_long$Condition == "Naming_NoInt")
#yRNI <- subset(df_long$Time, df_long$Condition == "Reading_NoInt")

# Make a vector of enpoints for the lines from residuals to the model means
model_means <- c(rep(meanRNI, length(xRNI)), rep(meanNI, length(xNI)), rep(meanNNI, length(xNNI)), rep(meanRI, length(xRI)))

# Plot the total sums of squares
p_TSS <- ggplot(df_long, aes(x = measurements, y = Time, color = Condition)) +
  geom_point() +
  geom_segment(aes(xend = measurements, yend = GrandMean), alpha = 0.2) +
  geom_hline(yintercept = GrandMean, linetype="dashed") +
  ylim(0,18) +
  theme_classic() +
  theme(legend.position="none") +
  labs(title = "Total Sums of Squares")

# Plot the residual sums of squares
p_RSS <- ggplot(df_long, aes(x = measurements, y = Time, color = Condition)) +
  geom_point() +
  geom_segment(aes(xend=measurements, yend=model_means), alpha = 0.2) +
  geom_segment(aes(x=min(xNI),xend=max(xNI),y= meanNI,yend=meanNI),colour="black") +
  geom_segment(aes(x=min(xRI),xend=max(xRI),y= meanRI,yend=meanRI),colour="black") +
  geom_segment(aes(x=min(xNNI),xend=max(xNNI),y= meanNNI,yend=meanNNI),colour="black") +
  geom_segment(aes(x=min(xRNI),xend=max(xRNI),y= meanRNI,yend=meanRNI),colour="black") +
  ylim(0,18) +
  theme_classic() +
  theme(legend.position="none") +
  labs(title = "Residual Sums of Squares")

# Plot the model sums of squares
p_MSS <- ggplot(df_long, aes(x = measurements, y = Time, color = Condition)) +
  geom_segment(aes(x=min(xNI),xend=max(xNI),y= meanNI,yend=meanNI),colour="black") +
  geom_segment(aes(x=min(xRI),xend=max(xRI),y= meanRI,yend=meanRI),colour="black") +
  geom_segment(aes(x=min(xNNI),xend=max(xNNI),y= meanNNI,yend=meanNNI),colour="black") +
  geom_segment(aes(x=min(xRNI),xend=max(xRNI),y= meanRNI,yend=meanRNI),colour="black") +
  geom_hline(yintercept = GrandMean, linetype="dashed") +
  geom_segment(aes(x=median(xNI), xend = median(xNI),y = meanNI, yend = GrandMean)) +
  geom_segment(aes(x=median(xRI), xend = median(xRI),y = meanRI, yend = GrandMean)) +
  geom_segment(aes(x=median(xNNI), xend = median(xNNI),y = meanNNI, yend = GrandMean)) +
  geom_segment(aes(x=median(xRNI), xend = median(xRNI),y = meanRNI, yend = GrandMean)) +
  ylim(0,18) +
  theme_classic() +
  theme(legend.position="none") +
  labs(title = "Model Sums of Squares")

# Make a three-panel plot
ggarrange(p_TSS, p_RSS, p_MSS, ncol = 3)

```


The F-value, the critical statistic in the table below, is the "Mean Square" of the model, dvided by the "Mean Square" of the residuals:
$F=\frac{MS_{model}}{MS_{residual}}$



```{r echo = T, results = 'false', message=FALSE, warning = F} 
# calculate ANOVA
library(lmerTest)

# Define our model. Here, we are predicting Time as modeled by Task, Interference, and the interaction of Task and Interference. We are modelling ID as a "random effect".
mod <- lmer(Time ~ Task + Interference + Task*Interference + (1|ID), data=df_long)
ans <- anova(mod)
```

### Display the ANOVA results in a nice table
```{r echo = T, results = 'false', message=FALSE, warning = F}
# Display the ANOVA results in a nice table
library(pander)
pander(ans)
```

### Print text that we can copy and paste into our report
```{r echo = T, results = 'false', message=FALSE, warning = F}
# Print text that we can copy and paste into our report
library(psycho)
res <- analyze(ans)
print(res)
```

### Display Estimates for Conditions
```{r echo = T, results = 'false', message=FALSE, warning = F}
# Display Estimates for Conditions
library(psycho)
library(pander)
mod_means <- get_means(mod)
pander(mod_means)
```

### Calculate post-hoc contrasts (Tukey)
```{r echo = T, results = 'false', message=FALSE, warning = F}
# Calculate post-hoc contrasts (Tukey)
post_hoc <- get_contrasts(mod)
pander(post_hoc[,c(1,3:6)])
```

